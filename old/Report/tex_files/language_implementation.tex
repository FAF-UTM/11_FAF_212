\chapter{Language Implementation}

\section{Language Lexing and Parsing}

Language implementation refers to the process of creating a program or system that can interpret or execute code written in a particular programming language.

The process of implementing a programming language involves several steps, including:

\begin{enumerate}
    \item Designing the language syntax and semantics: This involves defining the grammar, syntax rules, and semantics of the programming language.
    \item Creating a compiler or interpreter: A compiler is a program that translates the source code of a program written in the programming language into machine code that can be executed directly by a computer. An interpreter, on the other hand, executes the source code directly, without first translating it into machine code.
    \item Developing a runtime environment: This is the software environment in which a program written in the programming language can execute. It includes libraries, data structures, and other resources that are necessary for the program to run.
    \item Testing and debugging: Once the language implementation is complete, the compiler or interpreter must be tested and debugged to ensure that it works as expected.
    \item Documentation: Documentation is an important part of the language implementation. It includes user manuals, reference guides, and other documentation that helps programmers understand how to use the programming language.
\end{enumerate}

\vspace{1em}

\textbf{Parsing Algorithm}

Parsing algorithm: 
A common approach for parsing context-free grammar like this one is to use a technique called recursive descent parsing. Here is how it could be done for this grammar:

1. Start with the \texttt{<medical\_results>} rule.

2. Check if the next token matches \texttt{<test\_result>, <imaging\_result>, <lab\_result>}, or \texttt{<query>}.

3. If it matches one of those rules, call the corresponding parsing function.

4. If it doesn't match any of those rules, return an error.

5. The parsing function for \texttt{<test\_result>} would look for the "test" keyword, then call the \texttt{<test\_name>, <result>, and <unit>} parsing functions in order. It would also check if there is a \texttt{<reference\_range>} and parse it if it's there.

6. The parsing function for \texttt{<imaging\_result>} would look for the \texttt{"imaging" keyword}, then call the \texttt{<imaging\_type>, <result>}, and \texttt{<unit>} parsing functions in order. It would also check if there is an \texttt{<image\_location>} and parse it if it's there.

7. The parsing function for \texttt{<lab\_result>} would look for the \texttt{"lab"} keyword, then call the \texttt{<lab\_name>, <result>}, and \texttt{<unit>} parsing functions in order. It would also check if there is a \texttt{<reference\_range>} and parse it if it's there.

8. The parsing function for \texttt{<query>} would look for the "show" keyword, then call the \texttt{<query\_type>} and \texttt{<patient\_data>} parsing functions in order.

9. The parsing function for \texttt{<query\_type>} would look for either \texttt{"all tests", "imaging", "labs", "average tests"}, or \texttt{"average imaging"}, and parse the associated data if necessary.

10. The parsing function for \texttt{<patient\_data>} would look for either "patient" and a patient ID, or "age" and a date of birth.

11. The parsing function for \texttt{<patient\_id>} would parse any string of alphanumeric characters.

12. The parsing function for \texttt{<birthday>} would look for a date in the format \texttt{"YYYY-MM-DD"}.

13. The parsing function for \texttt{<test\_name>, <imaging\_type>, <lab\_name>, <number>, <unit>, <normal\_range>, <high\_range>, and <low\_range>} would all parse their respective data according to the grammar rules.

Recursive descent parsing works by recursively calling parsing functions for each rule in the grammar until the entire input is parsed. If the input is valid according to the grammar, a parse tree is constructed that represents the structure of the input. If the input is not valid, an error is returned.

\textbf{Parser}

Parsing is the process of taking some text and figuring out what it means according to a set of rules. In this case, we have a grammar that describes the structure of medical test results, and we want to be able to read some text and figure out what kinds of results are being reported, what the values are, and what the reference ranges are.

The algorithm I described is called "recursive descent parsing." This means that we start at the top level of the grammar (the rule) and recursively call parsing functions for each sub-rule until we have fully parsed the input.

To do this, we start by checking the first word or symbol in the input to see if it matches any of the rules we have defined. If it does, we call the corresponding parsing function. For example, if the input starts with "test," we know we need to call the parsing function. That parsing function will then look for the "test" keyword, followed by the name of the test, the result value, and any reference range information.

If the input doesn't match any of our defined rules, we know there is a problem with the input and we report an error.

We can use this approach to parse any text that conforms to our grammar. This could be a single test result, a series of results, or even a full report that includes results from multiple patients.

\textbf{textx}

textx is a Python library that allows you to define domain-specific languages (DSLs) using a textual syntax. Once you've defined your DSL, you can use textx to parse input text that follows your DSL syntax and generate corresponding models or objects.

\begin{lstlisting}
// file.py
from textx import metamodel_from_file

# Define the metamodel using the grammar file
mm = metamodel_from_file('grammar.tx')

# Load the model from the input file
model = mm.model_from_file('test.med')

# Print the contents of the model
print(model.__dict__)
\end{lstlisting}

\textbf{grammar.tx}
\begin{lstlisting}
MedicalResults:
    (test_results[TestResult] | imaging_results[ImagingResult] | lab_results[LabResult] | queries[Query])*;

TestResult:
    'test' test_name=ID ':' result=FLOAT 
    ('units' unit=ID)? (reference_range=ReferenceRange)?;

ImagingResult:
    'imaging' imaging_type=ID ':' result=FLOAT 
    ('units' unit=ID)? (image_location=ImageLocation)?;

LabResult:
    'lab' lab_name=ID ':' result=FLOAT 
    ('units' unit=ID)? (reference_range=ReferenceRange)?;

Query:
    'show' query_type=QueryType 'for' patient_data=PatientData;

QueryType:
    ('all' result_type=ResultType | 'average' result_type=ResultType 'for' time_frame=ID);

ResultType:
    'tests' | 'imaging' | 'labs';

PatientData:
    ('patient' patient_id=ID | 'age' birthday=Date);

ReferenceRange:
    ('normal range' normal_range=Range | 'high range' high_range=FLOAT unit=ID 
    | 'low range' low_range=FLOAT unit=ID);

Range:
    lower=FLOAT '-' upper=FLOAT unit=ID;

ImageLocation:
    'location' location=ID;

Date:
    year=INT '-' month=INT '-' day=INT;

terminal FLOAT: /[-+]?(\d*\.\d+|\d+\.?\d*)/;
terminal INT: /[0-9]+/;
terminal ID: /[a-zA-Z_][a-zA-Z0-9_]*/;
\end{lstlisting}

\textbf{test.med}
\begin{lstlisting}
    Description { 
	test glucose= 120 
	units= 'mg/dL' 
	stage= 'normal' 
	range= '70-100'
}
Setting {
	patient= 'John Smith', 
	age= '1978-01-01'
	gender= 'male',
	weight= '175 lbs',
	height= '5 11'
}
Response {
    	show all tests for patient age 1978-01-01
   	 // Output: glucose: 120 mg/dL (normal)
}
\end{lstlisting}

This code defines a metamodel using the grammar.tx file, loads the model from the test.med file, and then accesses the contents of the model. The print statements at the end show some of the contents of the model, but you can access any of the other attributes or sub-attributes of the model as needed. Note that the output of this code will depend on the contents of your input file.

\textbf{final text.py}
\begin{lstlisting}
# Import the metamodel_from_file function from the textx library
from textx import metamodel_from_file

# Define the metamodel using the grammar file
mm = metamodel_from_file('grammar.tx')

# Load the model from the input file
model = mm.model_from_file('test.med')

# Access the contents of the model

# Get the Description object from the model
description = model.Description

# Get the glucose, units, stage, and reference range attributes from the Description object
glucose = description.glucose
units = description.units
stage = description.stage
reference_range = description.range

# Get the Setting object from the model
setting = model.Setting

# Get the patient, age, gender, weight, and height attributes from the Setting object
patient = setting.patient
age = setting.age
gender = setting.gender
weight = setting.weight
height = setting.height

# Get the Response object from the model
response = model.Response

# Get the query_type and patient_data attributes from the Response object
query_type = response.query_type
patient_data = response.patient_data

# Print some of the contents of the model
print(f"Patient: {patient}, Age: {age}")
print(f"Test: glucose={glucose}, units={units}, stage={stage}, reference range={reference_range}")
print(f"Query: query_type={query_type}, patient_data={patient_data}")
\end{lstlisting}

A brief overview of the process used by textx to parse input text using our specified grammar:
\begin{enumerate}
    \item Define the grammar: First, you define the grammar for your domain-specific language (DSL) using textx. The grammar specifies the syntax and structure of valid input text for your DSL, and it defines the rules for how to parse that input text into a model or object.
    \item Define a metamodel: Once you've defined your grammar, you use textx to define a metamodel based on that grammar. The metamodel provides a Pythonic interface for working with your DSL, and it contains information about the syntax and structure of your DSL, as well as the rules for parsing input text.
    \item Load the model: To parse input text and generate a corresponding model or object, you load the input text using the metamodel. textx takes care of the parsing process for you, using the rules defined in your grammar and metamodel to generate the model.
    \item Access the model: Once you've loaded the model, you can access its contents using Python code. The exact structure and contents of the model will depend on your DSL and the input text that you provided.
\end{enumerate}

Overall, textx provides a flexible and powerful way to define and parse DSLs using Python code. It abstracts away much of the complexity of parsing input text and generates a corresponding model that you can work with using Python code.



\textbf{tokenizer.py}

'Tokenizer' is the main class that performs the tokenization. It takes a string input as a parameter, which is the text to tokenize. It has several methods:

\texttt{\_\_init\_\_}
: Initializes the object with the input text and sets the initial positions of the cursor and read\_cursor.

read\_char: Reads the next character from the input text and updates the cursor and read\_cursor positions.

next\_token: Reads the next token from the input text and returns a Token object.

read\_identifier: Reads an identifier (e.g., a variable or function name) from the input text and returns it as a string.

read\_number: Reads a number (integer) from the input text and returns it as a string.

consume\_whitespace: Skips any whitespace characters (e.g., spaces, tabs, newlines) until a non-whitespace character is found.

\begin{lstlisting}
    import tokens as token

class Token:

    def __init__(self, type: str, literal: str = None) -> any:
        # Define the type and literal of each token
        self.type = type
        self.literal = literal

    def __str__(self) -> str:
         # Define how to represent each token as a string
        if self.literal:
            return f'Type {self.type} : Literal {self.literal}'
        return f'Type {self.type}'


class Tokenizer:
    def __init__(self, input: str) -> any:
        input (after current char)
        self.input: str = input  # input text
        self.cursor: int = 0
        self.read_cursor: int = 0
        self.ch = ''  # current character under examination, need to see how to get byte or rune type in python3
        self.read_char()

    def read_char(self) -> any:

        if self.read_cursor >= len(self.input):
            self.ch = 0
        else:
            self.ch = self.input[self.read_cursor]
        self.cursor = self.read_cursor
        self.read_cursor += 1

    def next_token(self) -> Token:
        tok: Token = Token(token.EOF, "")
        self.consume_whitespace()

        if self.ch == "=":
            tok = Token(token.ASSIGN)
        elif self.ch == '+':
            tok = Token(token.PLUS)
        elif self.ch == ':':
            tok = Token(token.COLON)
        elif self.ch == ';':
            tok = Token(token.SEMICOLON)
        elif self.ch == ',':
            tok = Token(token.COMMA)
        elif self.ch == '.':
            tok = Token(token.DOT)
        elif self.ch == '(':
            tok = Token(token.LPAREN)
        elif self.ch == ')':
            tok = Token(token.RPAREN)
        elif self.ch == '{':
            tok = Token(token.LBRACE)
        elif self.ch == '}':
            tok = Token(token.RBRACE)
        elif self.ch == 0:
            tok = Token(token.EOF)
        else:
            if is_letter(self.ch):
                tok.literal = self.read_identifier()
                tok.type = token.lookup_ident(tok.literal)
                return Token(tok.type, tok.literal)
            if is_digit(self.ch) and self.ch != '0':
                tok.literal = self.read_number()
                tok.type = token.INT
                return Token(token.INT, tok.literal)
            tok = Token(token.ILLEGAL, self.ch)
        self.read_char()
        return tok

    def read_identifier(self) -> str:
        cursor: int = self.cursor
        while is_letter(self.ch) or is_digit(self.ch):
            self.read_char()
        return self.input[cursor: self.cursor]

    def read_number(self) -> str:
        cursor: int = self.cursor
        while is_digit(self.ch):
            self.read_char()
        return self.input[cursor: self.cursor]

    def consume_whitespace(self) -> None:
        while self.ch == ' ' or self.ch == '\t' or self.ch == '\n' or self.ch == '\r':
            self.read_char()

def is_letter(ch: str) -> bool:
    return 'a' <= ch <= 'z' or 'A' <= ch <= 'Z' or ch == '_'

def is_digit(ch: str) -> bool:
    return '0' <= ch <= '9'
\end{lstlisting}

The main objective of utilizing the \texttt{<read\_char>} method is to obtain the subsequent character from the input and subsequently 
    move ahead in our position within the input. This method plays a crucial role in ensuring that we can access each character 
    of the input sequentially while maintaining the correct order of characters. By calling the \texttt{<read\_char>} method, we are able 
    to progress through the input data and retrieve each character in turn, enabling us to perform various operations on the 
    data as required.

\textbf{tokens.py}

\begin{lstlisting}
ILLEGAL = "ILLEGAL"
EOF = "EOF"

# Identifiers + literals
IDENT = "IDENT"  # x, y, test, temp, ...
INT = "INT"  # 123456789

# OPERATORS
ASSIGN = "="
PLUS = "+"

# Delimiters
COMMA = ","
DOT = "."
SEMICOLON = ";"

LPAREN = "("
RPAREN = ")"
LBRACE = "{"
RBRACE = "}"
COLON = ':'

# Keywords
FUNCTION = "FUNCTION"
LET = "LET"

keywords: dict = {
    "FUNCTION": FUNCTION,
    "LET": LET
}

def lookup_ident(ident: str):
    if ident in keywords:
        return keywords[ident]
    return IDENT

# Keywords
FUNCTION = "FUNCTION"
LET = "LET"

'''
class Token:
    def __init__(self, Type, Literal):
        self.Type = Type
        self.Literal = Literal

TokenType = str
'''
\end{lstlisting}

The code provided defines a basic Token class and a set of constants representing different types of tokens. The Token class has two attributes, Type and Literal, which represent the type and value of the token.

In addition to the Token class, the code defines a dictionary of keywords and a function called \texttt{<lookup\_ident>} that takes an identifier string as input and returns the corresponding token type (either IDENT or one of the keyword types).

This code above is a simple implementation of a lexer (also known as a tokenizer or scanner) for a programming language. A lexer takes a stream of characters (representing the source code of a program) and breaks it up into tokens, which are the basic building blocks of the language. Each token represents a single element of the program, such as a keyword, identifier, or operator. The tokens produced by the lexer are then passed on to the parser, which uses them to construct an abstract syntax tree representing the structure of the program.