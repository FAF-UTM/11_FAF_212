\chapter{Language Implementation}

\section{Host Language Selection - Python}

In the implementation of the DSL for analyzing medical data, Python was chosen as the host language due to several compelling reasons. This section will provide a description of the reasons behind selecting Python and briefly discuss alternative options considered.

\begin{enumerate}
  \item Popularity and Ecosystem:
    \begin{itemize}
      \item Python is one of the most popular programming languages worldwide, widely adopted in various domains, including scientific computing and data analysis. 
      \item Its popularity ensures a vast and active community of developers, extensive documentation, and a rich ecosystem of libraries and frameworks.
      \item Python's ecosystem offers a wide range of tools and resources specifically tailored for medical data analysis, making it a natural choice for implementing the DSL.
    \end{itemize}

  \item Readability and Simplicity:
    \begin{itemize}
      \item Python is renowned for its clean and readable syntax, allowing developers to express complex ideas and algorithms concisely.
      \item Its simplicity enhances code maintainability and readability, which is crucial for collaboration among interdisciplinary teams, including clinicians and researchers.
      \item Python's ease of understanding makes it accessible to a diverse range of stakeholders, promoting effective communication and collaboration in medical data analysis.
    \end{itemize}

  \item Data Analysis Libraries:
    \begin{itemize}
      \item Python boasts a robust collection of libraries for scientific computing and data analysis, such as NumPy, Pandas, SciPy, and scikit-learn.
      \item These libraries provide powerful and efficient tools for data manipulation, statistical analysis, machine learning, and visualization.
      \item Leveraging these libraries within the DSL simplifies the development process and enables users to perform advanced medical data analysis tasks seamlessly.
    \end{itemize}

  \item Integration Capabilities:
    \begin{itemize}
      \item Python excels in its ability to integrate with other languages, systems, and tools.
      \item It seamlessly integrates with C/C++ libraries through wrappers like Cython or ctypes, enabling high-performance computation when needed.
      \item Python also supports integration with databases, web frameworks, and visualization tools, facilitating data exchange and collaboration in the medical analysis workflow.
    \end{itemize}

\end{enumerate}

Alternative options were considered during the evaluation process, but after careful consideration, Python was selected as the most suitable host language for implementing the DSL for analyzing medical data, taking into account its popularity, readability, extensive ecosystem, powerful data analysis libraries, and integration capabilities.


\vspace{0.5cm}
\section{Language Lexing and Parsing}

Language implementation refers to the process of creating a program or system that can interpret or execute code written in a particular programming language.

The process of implementing a programming language involves several steps, including:

\begin{enumerate}
    \item Designing the language syntax and semantics: This involves defining the grammar, syntax rules, and semantics of the programming language.
    \item Creating a compiler or interpreter: A compiler is a program that translates the source code of a program written in the programming language into machine code that can be executed directly by a computer. An interpreter, on the other hand, executes the source code directly, without first translating it into machine code.
    \item Developing a runtime environment: This is the software environment in which a program written in the programming language can execute. It includes libraries, data structures, and other resources that are necessary for the program to run.
    \item Testing and debugging: Once the language implementation is complete, the compiler or interpreter must be tested and debugged to ensure that it works as expected.
    \item Documentation: Documentation is an important part of the language implementation. It includes user manuals, reference guides, and other documentation that helps programmers understand how to use the programming language.
\end{enumerate}

\vspace{1em}

\textbf{Parsing Algorithm}

Parsing algorithm: 
A common approach for parsing context-free grammar like this one is to use a technique called recursive descent parsing. Here is how it could be done for this grammar:

1. Start with the \texttt{<medical\_results>} rule.

2. Check if the next token matches \texttt{<test\_result>, <imaging\_result>, <lab\_result>}, or \texttt{<query>}.

3. If it matches one of those rules, call the corresponding parsing function.

4. If it doesn't match any of those rules, return an error.

5. The parsing function for \texttt{<test\_result>} would look for the "test" keyword, then call the \texttt{<test\_name>, <result>, and <unit>} parsing functions in order. It would also check if there is a \texttt{<reference\_range>} and parse it if it's there.

6. The parsing function for \texttt{<imaging\_result>} would look for the \texttt{"imaging" keyword}, then call the \texttt{<imaging\_type>, <result>}, and \texttt{<unit>} parsing functions in order. It would also check if there is an \texttt{<image\_location>} and parse it if it's there.

7. The parsing function for \texttt{<lab\_result>} would look for the \texttt{"lab"} keyword, then call the \texttt{<lab\_name>, <result>}, and \texttt{<unit>} parsing functions in order. It would also check if there is a \texttt{<reference\_range>} and parse it if it's there.

8. The parsing function for \texttt{<query>} would look for the "show" keyword, then call the \texttt{<query\_type>} and \texttt{<patient\_data>} parsing functions in order.

9. The parsing function for \texttt{<query\_type>} would look for either \texttt{"all tests", "imaging", "labs", "average tests"}, or \texttt{"average imaging"}, and parse the associated data if necessary.

10. The parsing function for \texttt{<patient\_data>} would look for either "patient" and a patient ID, or "age" and a date of birth.

11. The parsing function for \texttt{<patient\_id>} would parse any string of alphanumeric characters.

12. The parsing function for \texttt{<birthday>} would look for a date in the format \texttt{"YYYY-MM-DD"}.

13. The parsing function for \texttt{<test\_name>, <imaging\_type>, <lab\_name>, <number>, <unit>, <normal\_range>, <high\_range>, and <low\_range>} would all parse their respective data according to the grammar rules.

Recursive descent parsing works by recursively calling parsing functions for each rule in the grammar until the entire input is parsed. If the input is valid according to the grammar, a parse tree is constructed that represents the structure of the input. If the input is not valid, an error is returned.

\textbf{Parser}

Parsing is the process of taking some text and figuring out what it means according to a set of rules. In this case, we have a grammar that describes the structure of medical test results, and we want to be able to read some text and figure out what kinds of results are being reported, what the values are, and what the reference ranges are.

The algorithm I described is called "recursive descent parsing." This means that we start at the top level of the grammar (the rule) and recursively call parsing functions for each sub-rule until we have fully parsed the input.

To do this, we start by checking the first word or symbol in the input to see if it matches any of the rules we have defined. If it does, we call the corresponding parsing function. For example, if the input starts with "test," we know we need to call the parsing function. That parsing function will then look for the "test" keyword, followed by the name of the test, the result value, and any reference range information.

If the input doesn't match any of our defined rules, we know there is a problem with the input and we report an error.

We can use this approach to parse any text that conforms to our grammar. This could be a single test result, a series of results, or even a full report that includes results from multiple patients.
\begin{lstlisting}
    from lexer import Lexer, IDENT, STRING, NUMBER, COLON, NEWLINE, EOF

class Parser:
    def __init__(self, lexer):
        self.lexer = lexer
        self.current_token = self.lexer.get_next_token()

    def error(self):
        raise Exception('Invalid syntax')

    def eat(self, token_type):
        if self.current_token.type == token_type:
            self.current_token = self.lexer.get_next_token()
        else:
            self.error()

    def parse(self):
        results = {}

        while self.current_token.type != EOF:
            if self.current_token.type == IDENT:
                ident = self.current_token.literal
                self.eat(IDENT)

                self.eat(COLON)

                if self.current_token.type == NUMBER:
                    results[ident] = self.current_token.literal
                    self.eat(NUMBER)
                elif self.current_token.type == STRING:
                    results[ident] = self.current_token.literal
                    self.eat(STRING)

                self.eat(NEWLINE)

        return results
\end{lstlisting}

The provided code defines a lexer and a parser class for processing a specific DSL (Domain-Specific Language) syntax.

The lexer tokenizes the input code into specific token types, such as \texttt{IDENT} (identifier), \texttt{STRING} (string literal), \texttt{NUMBER} (numeric literal), \texttt{COLON} (colon), \texttt{NEWLINE} (line break), and \texttt{EOF} (end of file).

The parser class takes an instance of the lexer and performs syntax analysis on the tokenized input. It implements the parsing logic for the DSL syntax, which seems to follow a key-value pair format with identifiers and corresponding values.

The \texttt{parse} method of the parser class iterates through the tokens and extracts the key-value pairs. It expects the input to follow the pattern: \texttt{<identifier>: <value>}, where the value can be either a number or a string. The identified key-value pairs are stored in a dictionary called \texttt{results}.

If the input code conforms to the expected syntax, the \texttt{parse} method returns the \texttt{results} dictionary containing the parsed key-value pairs. Otherwise, an exception is raised for invalid syntax.



\textbf{textx}

textx is a Python library that allows you to define domain-specific languages (DSLs) using a textual syntax. Once you've defined your DSL, you can use textx to parse input text that follows your DSL syntax and generate corresponding models or objects.

\begin{lstlisting}
// file.py
from textx import metamodel_from_file

# Define the metamodel using the grammar file
mm = metamodel_from_file('grammar.tx')

# Load the model from the input file
model = mm.model_from_file('test.med')

# Print the contents of the model
print(model.__dict__)
\end{lstlisting}

\textbf{grammar.tx}
\begin{lstlisting}
MedicalResults:
    (test_results[TestResult] | imaging_results[ImagingResult] | lab_results[LabResult] | queries[Query])*;

TestResult:
    'test' test_name=ID ':' result=FLOAT 
    ('units' unit=ID)? (reference_range=ReferenceRange)?;

ImagingResult:
    'imaging' imaging_type=ID ':' result=FLOAT 
    ('units' unit=ID)? (image_location=ImageLocation)?;

LabResult:
    'lab' lab_name=ID ':' result=FLOAT 
    ('units' unit=ID)? (reference_range=ReferenceRange)?;

Query:
    'show' query_type=QueryType 'for' patient_data=PatientData;

QueryType:
    ('all' result_type=ResultType | 'average' result_type=ResultType 'for' time_frame=ID);

ResultType:
    'tests' | 'imaging' | 'labs';

PatientData:
    ('patient' patient_id=ID | 'age' birthday=Date);

ReferenceRange:
    ('normal range' normal_range=Range | 'high range' high_range=FLOAT unit=ID 
    | 'low range' low_range=FLOAT unit=ID);

Range:
    lower=FLOAT '-' upper=FLOAT unit=ID;

ImageLocation:
    'location' location=ID;

Date:
    year=INT '-' month=INT '-' day=INT;

terminal FLOAT: /[-+]?(\d*\.\d+|\d+\.?\d*)/;
terminal INT: /[0-9]+/;
terminal ID: /[a-zA-Z_][a-zA-Z0-9_]*/;
\end{lstlisting}

\textbf{test.med}
\begin{lstlisting}
    Description { 
	test glucose= 120 
	units= 'mg/dL' 
	stage= 'normal' 
	range= '70-100'
}
Setting {
	patient= 'John Smith', 
	age= '1978-01-01'
	gender= 'male',
	weight= '175 lbs',
	height= '5 11'
}
Response {
    	show all tests for patient age 1978-01-01
   	 // Output: glucose: 120 mg/dL (normal)
}
\end{lstlisting}

This code defines a metamodel using the grammar.tx file, loads the model from the test.med file, and then accesses the contents of the model. The print statements at the end show some of the contents of the model, but you can access any of the other attributes or sub-attributes of the model as needed. Note that the output of this code will depend on the contents of your input file.

\textbf{final text.py}
\begin{lstlisting}
# Import the metamodel_from_file function from the textx library
from textx import metamodel_from_file

# Define the metamodel using the grammar file
mm = metamodel_from_file('grammar.tx')

# Load the model from the input file
model = mm.model_from_file('test.med')

# Access the contents of the model

# Get the Description object from the model
description = model.Description

# Get the glucose, units, stage, and reference range attributes from the Description object
glucose = description.glucose
units = description.units
stage = description.stage
reference_range = description.range

# Get the Setting object from the model
setting = model.Setting

# Get the patient, age, gender, weight, and height attributes from the Setting object
patient = setting.patient
age = setting.age
gender = setting.gender
weight = setting.weight
height = setting.height

# Get the Response object from the model
response = model.Response

# Get the query_type and patient_data attributes from the Response object
query_type = response.query_type
patient_data = response.patient_data

# Print some of the contents of the model
print(f"Patient: {patient}, Age: {age}")
print(f"Test: glucose={glucose}, units={units}, stage={stage}, reference range={reference_range}")
print(f"Query: query_type={query_type}, patient_data={patient_data}")
\end{lstlisting}

A brief overview of the process used by textx to parse input text using our specified grammar:
\begin{enumerate}
    \item Define the grammar: First, you define the grammar for your domain-specific language (DSL) using textx. The grammar specifies the syntax and structure of valid input text for your DSL, and it defines the rules for how to parse that input text into a model or object.
    \item Define a metamodel: Once you've defined your grammar, you use textx to define a metamodel based on that grammar. The metamodel provides a Pythonic interface for working with your DSL, and it contains information about the syntax and structure of your DSL, as well as the rules for parsing input text.
    \item Load the model: To parse input text and generate a corresponding model or object, you load the input text using the metamodel. textx takes care of the parsing process for you, using the rules defined in your grammar and metamodel to generate the model.
    \item Access the model: Once you've loaded the model, you can access its contents using Python code. The exact structure and contents of the model will depend on your DSL and the input text that you provided.
\end{enumerate}

Overall, textx provides a flexible and powerful way to define and parse DSLs using Python code. It abstracts away much of the complexity of parsing input text and generates a corresponding model that you can work with using Python code.



\textbf{tokenizer.py}

'Tokenizer' is the main class that performs the tokenization. It takes a string input as a parameter, which is the text to tokenize. It has several methods:

\texttt{\_\_init\_\_}
: Initializes the object with the input text and sets the initial positions of the cursor and read\_cursor.

read\_char: Reads the next character from the input text and updates the cursor and read\_cursor positions.

next\_token: Reads the next token from the input text and returns a Token object.

read\_identifier: Reads an identifier (e.g., a variable or function name) from the input text and returns it as a string.

read\_number: Reads a number (integer) from the input text and returns it as a string.

consume\_whitespace: Skips any whitespace characters (e.g., spaces, tabs, newlines) until a non-whitespace character is found.

\begin{lstlisting}
    import tokens as token

class Token:

    def __init__(self, type: str, literal: str = None) -> any:
        # Define the type and literal of each token
        self.type = type
        self.literal = literal

    def __str__(self) -> str:
         # Define how to represent each token as a string
        if self.literal:
            return f'Type {self.type} : Literal {self.literal}'
        return f'Type {self.type}'


class Tokenizer:
    def __init__(self, input: str) -> any:
        input (after current char)
        self.input: str = input  # input text
        self.cursor: int = 0
        self.read_cursor: int = 0
        self.ch = ''  # current character under examination, need to see how to get byte or rune type in python3
        self.read_char()

    def read_char(self) -> any:

        if self.read_cursor >= len(self.input):
            self.ch = 0
        else:
            self.ch = self.input[self.read_cursor]
        self.cursor = self.read_cursor
        self.read_cursor += 1

    def next_token(self) -> Token:
        tok: Token = Token(token.EOF, "")
        self.consume_whitespace()

        if self.ch == "=":
            tok = Token(token.ASSIGN)
        elif self.ch == '+':
            tok = Token(token.PLUS)
        elif self.ch == ':':
            tok = Token(token.COLON)
        elif self.ch == ';':
            tok = Token(token.SEMICOLON)
        elif self.ch == ',':
            tok = Token(token.COMMA)
        elif self.ch == '.':
            tok = Token(token.DOT)
        elif self.ch == '(':
            tok = Token(token.LPAREN)
        elif self.ch == ')':
            tok = Token(token.RPAREN)
        elif self.ch == '{':
            tok = Token(token.LBRACE)
        elif self.ch == '}':
            tok = Token(token.RBRACE)
        elif self.ch == 0:
            tok = Token(token.EOF)
        else:
            if is_letter(self.ch):
                tok.literal = self.read_identifier()
                tok.type = token.lookup_ident(tok.literal)
                return Token(tok.type, tok.literal)
            if is_digit(self.ch) and self.ch != '0':
                tok.literal = self.read_number()
                tok.type = token.INT
                return Token(token.INT, tok.literal)
            tok = Token(token.ILLEGAL, self.ch)
        self.read_char()
        return tok

    def read_identifier(self) -> str:
        cursor: int = self.cursor
        while is_letter(self.ch) or is_digit(self.ch):
            self.read_char()
        return self.input[cursor: self.cursor]

    def read_number(self) -> str:
        cursor: int = self.cursor
        while is_digit(self.ch):
            self.read_char()
        return self.input[cursor: self.cursor]

    def consume_whitespace(self) -> None:
        while self.ch == ' ' or self.ch == '\t' or self.ch == '\n' or self.ch == '\r':
            self.read_char()

def is_letter(ch: str) -> bool:
    return 'a' <= ch <= 'z' or 'A' <= ch <= 'Z' or ch == '_'

def is_digit(ch: str) -> bool:
    return '0' <= ch <= '9'
\end{lstlisting}

The main objective of utilizing the \texttt{<read\_char>} method is to obtain the subsequent character from the input and subsequently 
    move ahead in our position within the input. This method plays a crucial role in ensuring that we can access each character 
    of the input sequentially while maintaining the correct order of characters. By calling the \texttt{<read\_char>} method, we are able 
    to progress through the input data and retrieve each character in turn, enabling us to perform various operations on the 
    data as required.

\textbf{tokens.py}

\begin{lstlisting}
ILLEGAL = "ILLEGAL"
EOF = "EOF"

# Identifiers + literals
IDENT = "IDENT"  # x, y, test, temp, ...
INT = "INT"  # 123456789

# OPERATORS
ASSIGN = "="
PLUS = "+"

# Delimiters
COMMA = ","
DOT = "."
SEMICOLON = ";"

LPAREN = "("
RPAREN = ")"
LBRACE = "{"
RBRACE = "}"
COLON = ':'

# Keywords
FUNCTION = "FUNCTION"
LET = "LET"

keywords: dict = {
    "FUNCTION": FUNCTION,
    "LET": LET
}

def lookup_ident(ident: str):
    if ident in keywords:
        return keywords[ident]
    return IDENT

# Keywords
FUNCTION = "FUNCTION"
LET = "LET"

'''
class Token:
    def __init__(self, Type, Literal):
        self.Type = Type
        self.Literal = Literal

TokenType = str
'''
\end{lstlisting}

The code provided defines a basic Token class and a set of constants representing different types of tokens. The Token class has two attributes, Type and Literal, which represent the type and value of the token.

In addition to the Token class, the code defines a dictionary of keywords and a function called \texttt{<lookup\_ident>} that takes an identifier string as input and returns the corresponding token type (either IDENT or one of the keyword types).

This code above is a simple implementation of a lexer (also known as a tokenizer or scanner) for a programming language. A lexer takes a stream of characters (representing the source code of a program) and breaks it up into tokens, which are the basic building blocks of the language. Each token represents a single element of the program, such as a keyword, identifier, or operator. The tokens produced by the lexer are then passed on to the parser, which uses them to construct an abstract syntax tree representing the structure of the program.

The final version of the \texttt{lexer}

The lexer is responsible for tokenizing the input. It will recognize identifiers (like \texttt{patient}, \texttt{dob}, \texttt{test}), strings, numbers, and special symbols (like \texttt{:}, \texttt{\{}, \texttt{\}}, and newlines).

\texttt{IDENT}: Identifiers, like patient, dob, test, date, cbc, etc.

\texttt{STRING}: String literals, like John Doe, Complete Blood Count, etc.

\texttt{NUMBER}: Numeric literals, like 15.5, 45, 6000, etc.

\texttt{COLON}: The : symbol.

\texttt{NEWLINE}: Line breaks.


\begin{lstlisting}
# lexer.py
import re

IDENT = 'IDENT'
STRING = 'STRING'
NUMBER = 'NUMBER'
INTEGER = 'INTEGER'  # Add INTEGER token type
COLON = 'COLON'
NEWLINE = 'NEWLINE'
EOF = 'EOF'

class Token:
    def __init__(self, type, literal):
        self.type = type
        self.literal = literal

class Lexer:
    def __init__(self, input):
        self.input = input
        self.position = 0
        self.current_char = self.input[self.position]

    def error(self):
        raise Exception('Invalid character')

    def advance(self):
        self.position += 1
        if self.position > len(self.input) - 1:
            self.current_char = None  # Indicates end of input
        else:
            self.current_char = self.input[self.position]

    def skip_whitespace(self):
        while self.current_char is not None and self.current_char.isspace():
            self.advance()

    def ident(self):
        result = ''
        while self.current_char is not None and self.current_char.isalpha():
            result += self.current_char
            self.advance()
        return result

    def string(self):
        result = ''
        self.advance()  # Skip the initial quote
        while self.current_char is not None and self.current_char != '"':
            result += self.current_char
            self.advance()
        self.advance()  # Skip the closing quote
        return result

    def number(self):
        result = ''
        if self.current_char == '-':
            result += self.current_char
            self.advance()
        if self.current_char is None or not self.current_char.isdigit():
            self.error()

        while self.current_char is not None and self.current_char.isdigit():
            result += self.current_char
            self.advance()

        if self.current_char == '.':
            result += self.current_char
            self.advance()
            while (
                self.current_char is not None and
                self.current_char.isdigit()
            ):
                result += self.current_char
                self.advance()
            token_type = NUMBER
        else:
            token_type = INTEGER
        return float(result) if token_type == NUMBER else int(result)

    def get_next_token(self):
        while self.current_char is not None:

            if self.current_char.isspace():
                self.skip_whitespace()
                continue

            if self.current_char.isalpha():
                return Token(IDENT, self.ident())

            if self.current_char == '"':
                return Token(STRING, self.string())

            if self.current_char.isdigit() or self.current_char == '-':
                return Token(NUMBER, self.number())

            if self.current_char == ':':
                self.advance()
                return Token(COLON, ':')

            if self.current_char == '\n':
                self.advance()
                return Token(NEWLINE, '\n')

            self.error()

        return Token(EOF, None)
\end{lstlisting}

The provided code represents a simple lexer implemented in Python. The lexer tokenizes input text into various token types such as identifiers, strings, numbers, colons, newlines, and the end-of-file marker.

The lexer class (\texttt{Lexer}) contains the following methods:

\begin{itemize}
    \item \texttt{error()}: Raises an exception for an invalid character.
    \item \texttt{advance()}: Moves the current character pointer to the next character in the input.
    \item \texttt{skip\_whitespace()}: Skips any whitespace characters.
    \item \texttt{ident()}: Reads and returns an identifier token from the input.
    \item \texttt{string()}: Reads and returns a string token enclosed in double quotes from the input.
    \item \texttt{number()}: Reads and returns a number token from the input, handling both integers and floating-point numbers.
    \item \texttt{get\_next\_token()}: The main method of the lexer, which iterates over the input characters, identifies the token type, and returns the corresponding \texttt{Token} object.
\end{itemize}

The \texttt{Token} class represents a token with two attributes: \texttt{type} (the token type, e.g., \texttt{IDENT}, \texttt{STRING}, etc.) and \texttt{literal} (the actual value of the token).

To use this lexer, you can create an instance of the \texttt{Lexer} class, passing the input text as a parameter. Then, you can call the \texttt{get\_next\_token()} method in a loop to retrieve the next token until the end-of-file token (\texttt{EOF}) is encountered.

Note that this lexer is a basic example and may not handle all possible cases or support more complex tokenization requirements. It can serve as a starting point for building a lexer for a DSL for analyzing medical data.


\vspace{0.5cm}
\section{DSL Implementation Examples}

Here's an example of how you could define a DSL implementation using the provided medical data:

\begin{lstlisting}
patient: John Doe
dob: 1990-01-01
test: Complete Blood Count
date: 2023-06-10
cbc:
    hemoglobin: 15.5
    hematocrit: 45
    wbc: 6000
    rbc: 5
    platelets: 150000
lipid:
    ldl: 130
    hdl: 60
    triglycerides: 150
cmp:
    glucose: 100
    sodium: 140
    potassium: 4.5
    chloride: 100
    bicarbonate: 24
    bun: 20
    creatinine: 1.0
    albumin: 4.5
    bilirubin: 1.0
    alp: 100
    alt: 30
    ast: 30
\end{lstlisting}

In this example, the DSL syntax includes the main sections of the medical data such as the patient information (\texttt{patient}, \texttt{dob}), the test information (\texttt{test, date}), and the specific test results (\texttt{cbc, lipid, cmp}). Each test section contains the respective attributes and their corresponding values.
